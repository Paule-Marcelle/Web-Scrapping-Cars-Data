# -*- coding: utf-8 -*-
"""Assignement_Web_scraping_Expat_Dakar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14WjPzIdBHPNcFVhnXpjpGvrU21UgmWz8

## **Web Scraping de données sur des voitures de type venant en vente**
"""



# Importer package
from requests import get
from bs4 import BeautifulSoup

import pandas as pd



# url de la page 1
Url1 = 'https://www.expat-dakar.com/voitures/dakar?condition=used-abroad'
res1 = get(Url1)

# Objet BeautifulSoup
soup1 = BeautifulSoup(res1.text)

# Récupérer le code html de tous les conteneurs de la page 1
conteneurs = soup1.find_all('div',class_='listings-cards__list-item')

len(conteneurs)

# Premier conteneur de la page 1
conteneur = conteneurs[0]
conteneur

# Collecter les  données contenues sur le premier conteneur de la page 1
Gen1 = conteneur.find('div',class_='listing-card__header__tags').find_all('span')
Ven_Occ = Gen1[0].text
Marque=Gen1[1].text
Année =  Gen1[2].text
Aut_Man =  Gen1[3].text
Adresse = conteneur.find('div',  class_="listing-card__header__location").text.replace('\n','')
Prix =conteneur.find('span',class_="listing-card__price__value 1").text.replace('\n','').replace('\u202f','').replace('F Cfa','')
Prix
# Creation du dictionnaire qui contiendra les données du premier conteneur
obj1 = {
           'Ven_Occ': Ven_Occ,
           'Marque': Marque,
           'Année': Année,
          'Aut_Man': Aut_Man,
           'Adresse': Adresse,
           'Prix': Prix
      }
obj1

# Collecter les données de tous les conteneurs  de la première page
data1 = []
for conteneur in conteneurs:
  try :
   # Collecter les  données contenues sur le premier conteneur de la page 1
    Gen1 = conteneur.find('div',class_='listing-card__header__tags').find_all('span')
    Ven_Occ = Gen1[0].text
    Marque=Gen1[1].text
    Année =  Gen1[2].text
    Aut_Man =  Gen1[3].text
    Adresse = conteneur.find('div',  class_="listing-card__header__location").text.replace('\n','')
    Prix =conteneur.find('span',class_="listing-card__price__value 1").text.replace('\n','').replace('\u202f','').replace('F Cfa','')
    Prix
    # Creation du dictionnaire qui contiendra les données du premier conteneur
    obj1 = {
              'Ven_Occ': Ven_Occ,
              'Marque': Marque,
              'Année': Année,
              'Aut_Man': Aut_Man,
              'Adresse': Adresse,
              'Prix': Prix
          }
    data1.append(obj1)
  except:
    pass

# Dataframe des données de la page 1
df1 = pd.DataFrame(data1)

# Collecter les données de la page 1 à la page 311
df1 = pd.DataFrame() # Dataframe final
for p in range(1,311):
   # Fournir le code
    Url1 = f'https://www.expat-dakar.com/voitures/dakar?condition=used-abroad&page={p}'
    res1 = get(Url1)
    soup1 = BeautifulSoup(res1.text)
    conteneurs = soup1.find_all('div',class_='listings-cards__list-item')
   # code fournier le code
    data1 = []
    for conteneur in conteneurs:
      try :
   # Collecter les  données contenues sur le premier conteneur de la page 1
        Gen1 = conteneur.find('div',class_='listing-card__header__tags').find_all('span')
        Ven_Occ = Gen1[0].text
        Marque=Gen1[1].text
        Année =  Gen1[2].text
        Aut_Man =  Gen1[3].text
        Adresse = conteneur.find('div',  class_="listing-card__header__location").text.replace('\n','')
        Prix =conteneur.find('span',class_="listing-card__price__value 1").text.replace('\n','').replace('\u202f','').replace('F Cfa','')

    # Creation du dictionnaire qui contiendra les données du premier conteneur
        obj1 = {
                'Ven_Occ': Ven_Occ,
                'Marque': Marque,
                'Année': Année,
                'Aut_Man': Aut_Man,
                'Adresse': Adresse,
                'Prix': Prix
            }
        data1.append(obj1)
      except:
        pass
    DF1 = pd.DataFrame(data1)
    df1 = pd.concat([df1,DF1],axis=0)

# Afficher les 5 premiers lignes de df1
df1.head()

# Afficher la dimension de df1
df1.shape

# Reset des index
df1.reset_index(drop = True, inplace = True )

df1

# Infos de données
df1.info()

"""## **Web Scraping de données sur des voitures de type occasion en vente**"""

# Url de la page 1
Url2 = 'https://www.expat-dakar.com/vehicules?condition=used'
res2 = get(Url2)

# Objet BeautifulSoup
soup2 = BeautifulSoup(res2.text)

# Récupérer le code html de tous les conteneurs de la page 1
Conteneurs = soup2.find_all('div',class_='listings-cards__list-item')

# Premier conteneur de la page 1
Conteneur = Conteneurs[0]
Conteneur

# Collecter les  données contenues sur le premier conteneur de la page 1
Gen2 = Conteneur.find('div',class_='listing-card__header__tags')  # Utiliser find et find_all pour récupérer le code html contenant les quatre données ( D'occasion, Marque, Année, Automtique ou Manuelle )
Ven_Occ = Gen2.find_all('span')[0].text
Marque=  Gen2.find_all('span')[1].text      # Marque de la voiture (Récupérer Marque en partant de Gen2)
Année =  Gen2.find_all('span')[2].text     # Année de la voiture (Récupérer Année en partant de Gen2)
#Aut_Man= Gen2.find_all('span')[3].text    # Voiture automatique ou manuelle (Récupérer Aut_Man en partant de Gen2)
Adresse = Conteneur.find('div',class_='listing-card__header__location').text.replace('\n','')     # Adresse
Prix = Conteneur.find('span',class_='listing-card__price__value 1').text.replace('\n','').replace('\u202f','')
# Creation du dictionnaire qui contiendra les données du premier conteneur
obj2 = {
           'Ven_Occ': Ven_Occ,
           'Marque': Marque,
           'Année': Année,
           'Aut_Man': Aut_Man,
           'Adresse': Adresse,
           'Prix': Prix
      }
obj2

# Collecter les données de tous les conteneurs  de la première page
data2 = []
for Conteneur in Conteneurs :
  try :
    Gen2 = Conteneur.find('div',class_='listing-card__header__tags')
    Marque=  Gen2.find_all('span')[1].text
    Année =  Gen2.find_all('span')[2].text
    Aut_Man = Gen2.find_all('span')[3].text
    Adresse = Conteneur.find('div',class_='listing-card__header__location').text.replace('\n','')     # Adresse
    Prix = Conteneur.find('span',class_='listing-card__price__value 1').text.replace('\n','').replace('\u202f','')
# Creation du dictionnaire qui contiendra les données du premier conteneur
    obj2 = {
           'Ven_Occ': Ven_Occ,
           'Marque': Marque,
           'Année': Année,
           'Aut_Man': Aut_Man,
           'Adresse': Adresse,
           'Prix': Prix
           }
    data2.append(obj2)
  except:
    pass

# DataFrame des données de la page 1
df2 = pd.DataFrame(data2)

# Collecter les données de la page 1 à la page 334
df2 = pd.DataFrame()
for p in range(1,334):
    Url2 = f'https://www.expat-dakar.com/voitures/dakar?condition=used-abroad&page={p}'
    res2 = get(Url1)
    soup1 = BeautifulSoup(res1.text)
    conteneurs = soup1.find_all('div',class_='listings-cards__list-item')
    data2 = []
    for Conteneur in Conteneurs :
      try :
        Gen2 = Conteneur.find('div',class_='listing-card__header__tags')
        Marque=  Gen2.find_all('span')[1].text
        Année =  Gen2.find_all('span')[2].text
        Aut_Man = Gen2.find_all('span')[3].text
        Adresse = Conteneur.find('div',class_='listing-card__header__location').text.replace('\n','')     # Adresse
        Prix = Conteneur.find('span',class_='listing-card__price__value 1').text.replace('\n','').replace('\u202f','')

        obj2 = {
            'Ven_Occ': Ven_Occ,
            'Marque': Marque,
            'Année': Année,
            'Aut_Man': Aut_Man,
            'Adresse': Adresse,
            'Prix': Prix
            }
        data2.append(obj2)
      except:
        pass
    DF2 = pd.DataFrame(data2)
    df2 = pd.concat([DF2,df2],axis=0)

# Afficher les 5 premiers lignes de df2
df2.head(100)

# Afficher la dimension de df2
df2.shape

# Reset des index de df2
df2.reset_index(drop = True, inplace = True )

df2

# Infos des données
df2.info

"""## Concater les deux dataframes des voitures venant avec celui des voitures D'occassion"""

# Concaténation de df1 et df2
df_concat = pd.concat([df1,df2],axis=0)

# Reset des index  de df_concat
df_concat.reset_index(drop = True, inplace = True )

# Afficher les 5 premiers lignes de df_concat
df_concat.head(6535)

# Afficher les infos de df_concat
df_concat.info()